# Face2Face

---
## Information
| Title |  Face2Face: Real-Time Face Capture and Reenactment of RGB Videos |
| -------------------------------------------- | ----------------------------------------------------- |
| `Conference or Journals`                                 | IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016                                          |
| `Date`                                  | 2016 |
| `Authors`                                   | Justus Thies, Michael ZollhÃ¶fer, Marc Stamminger, Christian Theobalt, Matthias NieÃŸner |
| `Authors' affiliation`                                |                               |
| `Keywords`                                |  |
| `project website`                                    | https://niessnerlab.org/projects/thies2016face.html                                          |
| `code`                           |  |
| `paper` | https://niessnerlab.org/papers/2016/1facetoface/thies2016face.pdf|
---

## Task Overview
### ðŸŽ¯ Goal
  - Real-time facial reenactment using only RGB video inputs.
### ðŸ“¥ Input:
  - Target video
    - Pre-recorded monocular video (e.g, Youtube video)
  - Source video: 
    - Monocular video captured live with a commodity webcam.
### ðŸ“¤  Output:
  - Animate the facial expressions of the target video by a source actor
  - Re-render the manipulated output video in a photo-realistic fashion

## ðŸ’¡ Motivations
- Instead of transferring facial expressions to virtual CG characters, our main contribution is **monocular facial reenactment** in **real-time**.
- Online transfer of facial expressions of a source actor captured by an RGB sensor to a target actor.
- Faithful photo-realistic facial reenactment is the foundation for a variety of applications;
    - for instance, in video conferencing, the video feed can be adapted to match the face motion of a translator, or face videos can be convincingly dubbed to a foreign language.


## ðŸ”§ How to Solve?
1. Optimize identity from video sequences (Offline / Non real-time)
2. facial expression tracking (Real-time)
3. Expression transfer for reenactment (Real-time)
4. re-render the synthesized target face (Real-time)
- ðŸ›  Detailed Breakdown:
    - Preprocessing, 
        - address the under-constrained problem of **facial identity recovery** from monocular
    video by **non-rigid model-based bundling**.
            - ðŸ“Œ Bundling: The process of jointly optimizing camera poses, 3D structure, and object identity by using information from multiple video frames.
    - â± Runtime, 
        - track facial expressions of both source and target video using a **dense photometric consistency measure**.
        - Reenactment is then achieved by fast and efficient **deformation transfer** between source and target.
        - **re-render** the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination
    - ðŸ‘„ Mouth interior
        - The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit.

## ðŸ§ª Contributions
- dense, global non-rigid model-based bundling
- accurate tracking, appearance, and lighting estimation
in unconstrained live RGB video
- person-dependent expression transfer using subspace
deformations
- and a novel mouth synthesis approach.

---

## ðŸ“ˆ Method
### 1.Synthesis of facial imagery

#### Multi-linear Face Model (Used in Face2Face)

Face2Face uses a **multi-linear 3D face model** to represent and manipulate facial geometry, appearance, and expression.

#### Model Structure
$\mathrm{Face}(x) = \mathbf{a} + \mathbf{E}_{\mathrm{id}} \cdot \boldsymbol{\alpha} + \mathbf{E}_{\mathrm{alb}} \cdot \boldsymbol{\beta} + \mathbf{E}_{\mathrm{exp}} \cdot \boldsymbol{\delta}$


| ìš”ì†Œ                                                      | ì„¤ëª…                             |
| ------------------------------------------------------- | ------------------------------ |
| $\mathbf{a}$                                            | í‰ê·  ì–¼êµ´ ëª¨ì–‘ (mean shape / albedo) |
| $\mathbf{E}_{\text{id}} \in \mathbb{R}^{3n \times 80}$  | ì •ì²´ì„±(identity) í˜•íƒœì˜ ì£¼ì„±ë¶„(ê¸°ì €)      |
| $\boldsymbol{\alpha} \in \mathbb{R}^{80}$               | ê°œë³„ ì‚¬ëžŒì˜ ì •ì²´ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê³„ìˆ˜            |
| $\mathbf{E}_{\text{alb}} \in \mathbb{R}^{3n \times 80}$ | í”¼ë¶€ ìƒ‰ìƒ(albedo)ì˜ ê¸°ì €              |
| $\boldsymbol{\beta} \in \mathbb{R}^{80}$                | ë°˜ì‚¬ìœ¨(albedo) ê³„ìˆ˜                 |
| $\mathbf{E}_{\text{exp}} \in \mathbb{R}^{3n \times 76}$ | í‘œì •(expression) ê¸°ì €              |
| $\boldsymbol{\delta} \in \mathbb{R}^{76}$               | í‘œì • ê³„ìˆ˜ (ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì )               |
The face is parameterized as a combination of:


* **Identity (shape):**
  - $\mathbf{a}_{\mathrm{id}} + \mathbf{E}_{\mathrm{id}} \cdot \boldsymbol{\alpha}$
  - $\mathbf{E}_{\text{id}} \in \mathbb{R}^{3n \times 80}$, $\boldsymbol{\alpha} \in \mathbb{R}^{80}$

* **Albedo (skin color):**
  - $\mathbf{a}_{\text{alb}} + \mathbf{E}_{\mathrm{alb}} \cdot \boldsymbol{\beta}$
  - $\mathbf{E}_{\text{alb}} \in \mathbb{R}^{3n \times 80}$, $\boldsymbol{\beta} \in \mathbb{R}^{80}$

* **Expression:**
  - $\mathbf{E}_{\text{exp}} \cdot \boldsymbol{\delta}$

  - $\mathbf{E}_{\text{exp}} \in \mathbb{R}^{3n \times 76}$, $\boldsymbol{\delta} \in \mathbb{R}^{76}$

> âœ… Total mesh: **53K vertices**, **106K faces**

---

#### Rendering Pipeline

* **Rigid transformation:** $\Phi(v)$, camera projection: $\Pi(v)$
* **Illumination:** Approximated using **Spherical Harmonics (SH)** (1st 3 bands)
* **Output image:** Rasterization of 3D face model under lighting and camera params

---

#### Parameters Summary

```text
P = { Î± (identity), Î² (albedo), Î´ (expression), Î³ (lighting), R, t (pose), Îº (camera) }
```

---

### 2. Energy Formulation

Face2Face reconstructs all unknown parameters $\mathbf{P}$ from monocular video via **robust variational optimization**.
The energy function is non-linear and composed of three terms:

---

#### Objective Function

$$
E(\mathbf{P}) = w_{\text{col}} \cdot E_{\text{col}} + w_{\text{lan}} \cdot E_{\text{lan}} + w_{\text{reg}} \cdot E_{\text{reg}}
$$

* $E_{\text{col}}$: Photometric consistency (pixel-wise appearance match)
* $E_{\text{lan}}$: Landmark alignment (feature points match)
* $E_{\text{reg}}$: Statistical regularization (parameter prior)

> **Weights:**
> $w_{\text{col}} = 1$, $w_{\text{lan}} = 10$, $w_{\text{reg}} = 2.5 \times 10^{-5}$

---

#### ðŸ–¼ï¸ Photo-Consistency $E_{\text{col}}$

Measures pixel-level difference between input and synthesized images:

$$
E_{\text{col}} = \sum_{p \in V} \| C_S(p) - C_I(p) \|_2
$$

* $C_S$: Synthesized image
* $C_I$: Input RGB image
* $V$: Set of visible pixels
* Uses **$\ell_{2,1}$-norm** for robustness (L2 per pixel, L1 over image)

---

#### ðŸŽ¯ Landmark Alignment $E_{\text{lan}}$

Aligns tracked 2D facial landmarks with projected 3D model points.

* Uses **facial landmark tracker** (Saragih et al.)
* Each 2D landmark $f_j$ maps to a 3D vertex $v_j = M_{\text{geo}}(\alpha, \delta)$
* Weighted by detection confidence $w_{\text{conf},j}$
* Helps guide optimization and avoid local minima

---

#### ðŸ“Š Statistical Regularization $E_{\text{reg}}$

Encourages parameters to stay near the mean of the multivariate normal prior:

- E_reg = || Î± / Ïƒ_id ||Â²â‚‚ + || Î² / Ïƒ_alb ||Â²â‚‚ + || Î´ / Ïƒ_exp ||Â²â‚‚



* Prevents **implausible facial geometry or reflectance**
* Keeps optimization **stable and realistic**

---

### 3. Data-Parallel Optimization

Face2Face solves a **nonlinear, unconstrained optimization problem** for facial tracking in **real time** using a **data-parallel GPU-based solver**.

---

#### âš™ï¸ Optimization Method

* **Objective:** Robust nonlinear tracking energy
* **Solver:** IRLS (Iteratively Reweighted Least Squares)
* **Linearization:** Each iteration transforms into a nonlinear least-squares problem.

---

#### ðŸ” Iterative Optimization Steps

1. **IRLS Iteration**

- â€– r(P) â€–â‚‚ = (â€– r(P_old) â€–â‚‚)â»Â¹ Â· â€– r(P) â€–â‚



* Fix residual weights
* Linearize objective
-  ê° ìš”ì†Œ ì„¤ëª…
    * $r(\mathcal{P})$: í˜„ìž¬ íŒŒë¼ë¯¸í„° $\mathcal{P}$ì—ì„œì˜ **ìž”ì°¨(residual)** ë²¡í„°
    * $r(\mathcal{P}_{\text{old}})$: ì´ì „ ë°˜ë³µ(iteration)ì—ì„œì˜ ìž”ì°¨ ë²¡í„°
    * $\| \cdot \|_2$: L2 ë…¸ë¦„ (ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬)
    * $\| \cdot \|_1$: L1 ë…¸ë¦„ (ì ˆëŒ“ê°’ í•©)
    * ê´„í˜¸ë¡œ ë¬¶ì¸ $\| r(\mathcal{P}_{\text{old}}) \|_2^{-1}$: ìƒìˆ˜ë¡œ ì·¨ê¸‰ â†’ **ê°€ì¤‘ì¹˜(weight)** ì—­í• 



2. **Gauss-Newton Step**

   * Solve:
$J^T J \delta^* = -J^T F$

   * Solver: **Preconditioned Conjugate Gradient (PCG)**
   * Output: Linear parameter update $\delta^*$

3. **Parameter Update**

   * Apply $\delta^*$ to update parameter vector $\mathbf{P}$

---

#### ðŸ§© Implementation Details

* **Jacobian $J$** and right-hand side $-J^T F$:
  Precomputed and stored in **GPU memory**
* **Framework:**

  * **DirectX 11** for rendering
  * **DirectCompute** for optimization
* **Optimization-friendly renderer:**

  * Uses **differential rasterization**
  * Stores **vertex & triangle attributes** to compute partial derivatives via **compute shaders**

---

### 4. Non-Rigid Model-Based Bundling

To robustly recover facial **identity** in a **monocular and under-constrained setting**, Face2Face applies a **non-rigid model-based bundling** strategy across multiple keyframes.

---

#### ðŸŽ¯ Purpose

* Estimate **global identity** (shape & albedo) and **intrinsics**
* Separate identity from **pose, expression, illumination**

---

#### ðŸ“¦ Estimated Parameters

For a set of $k$ keyframes, the following parameters are optimized:

| Type                  | Parameters                                                        |
| --------------------- | ----------------------------------------------------------------- |
| **Identity** (global) | $\alpha$, $\beta$ (shape, albedo)                                 |
| **Intrinsics**        | $\kappa$                                                          |
| **Per-frame**         | $\delta_k$, $R_k$, $t_k$, $\gamma_k$ (expression, pose, lighting) |

---

#### âš™ï¸ Optimization Strategy

* **Objective:** Joint energy across all keyframes
* **Solver:** PCG (Preconditioned Conjugate Gradient)
* **Jacobian:** Block-dense structure â†’ efficiently exploited
* **Hierarchical Gauss-Newton:**

  * 3 levels of resolution:

    * Coarse (25 iterations)
    * Medium (5)
    * Fine (1)
  * 4 PCG steps per iteration

> ðŸ” **Multilevel strategy** helps avoid local minima and improves convergence speed.

---

#### â±ï¸ Performance

* Keyframes used: $k = 6$
* Total processing time: **\~20 seconds**
* **Linear scaling** with number of keyframes

---

#### âœ… Key Advantage

> Enables robust identity recovery **without prior camera calibration**, even under varying **illumination, pose, and expression**.

---

### 5. Expression Transfer

To map **expressions from source to target** while maintaining each actorâ€™s personal facial traits, Face2Face uses a **subspace deformation transfer** technique.

---

#### ðŸŽ¯ Goal

> Transfer expressions in **real time**
> Preserve **person-specific expression characteristics**

---

#### ðŸ“ Method Overview

* Inspired by: **Sumner et al. (2004)** deformation transfer energy
* Works directly in the **expression blendshape subspace** (76D)
* Enables:

  * Fast computation
  * Precomputation of the **pseudo-inverse**
  * Low-dimensional optimization

---

#### ðŸ“Š Inputs and Outputs

| Role       | Data                                                           |
| ---------- | -------------------------------------------------------------- |
| **Input**  | Source identity $\alpha_S$, Target identity $\alpha_T$ (fixed) |
|            | Source expressions $\delta_S$: neutral + deformed              |
|            | Target neutral expression                                      |
| **Output** | Transferred expression $\delta_T$ in blendshape subspace       |

---

#### ðŸ§® Deformation Transfer Steps

1. **Compute deformation gradients**
   $A_i \in \mathbb{R}^{3 \times 3}$: per-triangle transformation for source (neutral â†’ deformed)

2. **Formulate a least-squares problem**
   Find $\delta_T$ that minimizes error in triangle edge deformation

$$
\min_{\delta_T} \| A \delta_T - b \|^2
$$

   * $A \in \mathbb{R}^{6|F| \times 76}$: fixed matrix from mesh edges and blendshape basis
   * $b \in \mathbb{R}^{6|F|}$: computed each frame on GPU, based on $\delta_S$

3. **Solve using precomputed pseudo-inverse**

   * Use SVD to precompute:

$$
\delta_T = A^+ b
$$
   * Final solve: small **76 Ã— 76** system â€” **real-time capable**

---

#### âœ… Advantages

* No need for extra smoothness terms (unlike Bouaziz et al.)
* Implicit smoothness via blendshape model
* Efficient GPU implementation
* **Plausible and person-specific expression output**


---

### 6. Mouth Retrieval

To generate a **realistic mouth region** for the reenacted face, Face2Face performs **image-based mouth retrieval and warping**.

---

#### ðŸŽ¯ Objective

> Synthesize a **photo-realistic** target mouth that matches the transferred expression

---

#### ðŸ§  Method

1. **Retrieve** the best-matching mouth image from the **target video sequence**
2. **Warp** the retrieved mouth region to fit the reenacted facial pose

---

#### ðŸ“¦ Assumptions

* The full target video or at least a **short segment** is available
* The target video contains **sufficient mouth shape variations**
* **Appearance of the target mouth is preserved**

  * âš ï¸ **No source mouth copy**
  * âš ï¸ **No generic 3D teeth model**

---

#### âœ… Advantages

* Maintains **person-specific realism**
* Avoids visual artifacts seen in:

  * **Mouth copy-paste** from source \[23]
  * **Generic 3D teeth proxies** \[8, 19]
* Produces **high-quality, natural-looking mouth synthesis**

---

> For full implementation details, refer to the original Face2Face paper (Section 10, Figure 4).


---
- **Since**: 2025.05.23  
- **Last updated**: 2025.05.23  
_(This .md file was written with the help of ChatGPT.)_
---
